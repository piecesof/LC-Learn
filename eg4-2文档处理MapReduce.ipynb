{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'该内容摘要如下：\\n\\n一篇发表于ICLR 2023会议的论文介绍了REACT模型，旨在提升语言模型在推理与行动方面的表现。作者包括Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan和Yuan Cao，分别来自普林斯顿大学计算机科学系和Google研究团队。该方法名为ReAct，结合了异常处理和动作执行的功能，通过与外部来源（如知识库或环境）交互来识别错误并获取更多信息。研究者将ReAct应用于多种语言理解和决策任务，并展示了其在基准模型上的优越性能，特别是在可解释性和可信度方面。\\n\\n此外，该内容还指出，ReAct能够生成比没有推理痕迹的基础模型更具可解释性的类人类任务解决轨迹。在两个互动决策基准（ALFWorld和WebShop）上，ReAct分别以绝对成功率34%和10%优于模仿学习和强化学习方法，即使是在没有详细提示的情况下。\\n\\n总结如下：\\n\\nREACT模型通过与外部资源交互来改进大型语言模型的推理与行动能力。该研究展示了ReAct在多个任务中的优越表现，并提高了模型的可解释性和可信度。特别是在互动决策任务中，ReAct表现出显著的优势，超过了模仿学习和强化学习方法。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, format_document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 加载arXiv上的论文ReAct: Synergizing Reasoning and Acting in Language Models\n",
    "loader = ArxivLoader(query = \"2210.03629\", load_max_docs = 1)\n",
    "docs = loader.load()\n",
    "\n",
    "# 把文本分割成500个字符为一组的片段\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "llm = ChatOllama(model = 'qwen2.5')\n",
    "\n",
    "# 构建工具函数：将Document转换成字符串\n",
    "document_prompt = PromptTemplate.from_template('{page_content}')\n",
    "partial_format_document = partial(format_document, prompt = document_prompt)\n",
    "\n",
    "# 构建Map链，对每个文档都先进行一轮总结\n",
    "map_chain = (\n",
    "    {'context': partial_format_document}\n",
    "    | PromptTemplate.from_template('Summarize this content(用中文): \\n\\n {context}')\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 构建Reduce链，合并之前的所有总结内容\n",
    "reduce_chain = (\n",
    "    {'context': lambda strs: '\\n\\n'.join(strs)}\n",
    "    | PromptTemplate.from_template('Combine these summaries(用中文): \\n\\n{context}')\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 把两个链合并成MapReduce链\n",
    "map_reduce = map_chain.map() | reduce_chain\n",
    "map_reduce.invoke(chunks[:4], config = {'max_concurrency': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test1",
   "language": "python",
   "name": "test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}